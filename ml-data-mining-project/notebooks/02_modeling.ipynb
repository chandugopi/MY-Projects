{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ¤– Machine Learning Models - Job Application Predictor\n",
                "\n",
                "This notebook builds and compares classification models to predict job application success."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# ML libraries\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.cluster import KMeans\n",
                "from xgboost import XGBClassifier\n",
                "from imblearn.over_sampling import SMOTE\n",
                "\n",
                "# Custom modules\n",
                "import sys\n",
                "sys.path.append('..')\n",
                "from src.preprocessing import preprocess_features, split_and_scale, handle_imbalance\n",
                "from src.evaluation import evaluate_model, plot_confusion_matrix, plot_roc_curves, create_comparison_table, plot_feature_importance\n",
                "from src.generate_data import generate_job_application_data\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Data Preparation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load or generate data\n",
                "try:\n",
                "    df = pd.read_csv('../data/job_applications.csv')\n",
                "except:\n",
                "    df = generate_job_application_data(n_samples=2000)\n",
                "    df.to_csv('../data/job_applications.csv', index=False)\n",
                "\n",
                "print(f\"Dataset Shape: {df.shape}\")\n",
                "print(f\"Target Distribution: {df['hired'].value_counts().to_dict()}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocess features\n",
                "X, y, feature_names = preprocess_features(df)\n",
                "print(f\"Features: {len(feature_names)}\")\n",
                "print(f\"X shape: {X.shape}, y shape: {y.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split and scale\n",
                "X_train, X_test, y_train, y_test, scaler = split_and_scale(X, y)\n",
                "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
                "print(f\"Train target distribution: {np.bincount(y_train)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Handle class imbalance with SMOTE\n",
                "X_train_smote, y_train_smote = handle_imbalance(X_train, y_train, method='smote')\n",
                "print(f\"After SMOTE: {X_train_smote.shape}\")\n",
                "print(f\"Resampled target distribution: {np.bincount(y_train_smote)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Model Training"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.1 Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Logistic Regression\n",
                "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr_model.fit(X_train_smote, y_train_smote)\n",
                "\n",
                "lr_metrics = evaluate_model(lr_model, X_test, y_test, 'Logistic Regression')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.2 Random Forest"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest\n",
                "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
                "rf_model.fit(X_train_smote, y_train_smote)\n",
                "\n",
                "rf_metrics = evaluate_model(rf_model, X_test, y_test, 'Random Forest')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2.3 XGBoost"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost\n",
                "xgb_model = XGBClassifier(\n",
                "    n_estimators=100,\n",
                "    max_depth=5,\n",
                "    learning_rate=0.1,\n",
                "    random_state=42,\n",
                "    use_label_encoder=False,\n",
                "    eval_metric='logloss'\n",
                ")\n",
                "xgb_model.fit(X_train_smote, y_train_smote)\n",
                "\n",
                "xgb_metrics = evaluate_model(xgb_model, X_test, y_test, 'XGBoost')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "all_metrics = [lr_metrics, rf_metrics, xgb_metrics]\n",
                "comparison_df = create_comparison_table(all_metrics)\n",
                "comparison_df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize comparison\n",
                "comparison_df_plot = comparison_df.drop('roc_auc', axis=1, errors='ignore')\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "comparison_df_plot.plot(kind='bar', ax=ax, colormap='viridis')\n",
                "plt.title('Model Performance Comparison')\n",
                "plt.ylabel('Score')\n",
                "plt.xlabel('Model')\n",
                "plt.legend(loc='lower right')\n",
                "plt.xticks(rotation=0)\n",
                "plt.ylim(0, 1)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ROC Curves\n",
                "models = {\n",
                "    'Logistic Regression': lr_model,\n",
                "    'Random Forest': rf_model,\n",
                "    'XGBoost': xgb_model\n",
                "}\n",
                "\n",
                "plot_roc_curves(models, X_test, y_test)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# XGBoost feature importance\n",
                "importance_df = pd.DataFrame({\n",
                "    'feature': feature_names,\n",
                "    'importance': xgb_model.feature_importances_\n",
                "}).sort_values('importance', ascending=False)\n",
                "\n",
                "plot_feature_importance(importance_df, top_n=15, title='XGBoost Feature Importance')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Top 10 features\n",
                "print(\"Top 10 Most Important Features:\")\n",
                "importance_df.head(10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. K-Means Clustering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find optimal K using Elbow Method\n",
                "inertias = []\n",
                "K_range = range(2, 11)\n",
                "\n",
                "for k in K_range:\n",
                "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
                "    kmeans.fit(X_train)\n",
                "    inertias.append(kmeans.inertia_)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(K_range, inertias, 'bo-')\n",
                "plt.xlabel('Number of Clusters (K)')\n",
                "plt.ylabel('Inertia')\n",
                "plt.title('Elbow Method for Optimal K')\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply K-Means with optimal K\n",
                "optimal_k = 3\n",
                "kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
                "cluster_labels = kmeans_final.fit_predict(X_train)\n",
                "\n",
                "print(f\"Cluster sizes: {np.bincount(cluster_labels)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze clusters\n",
                "# Add cluster labels to training data\n",
                "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
                "X_train_df['cluster'] = cluster_labels\n",
                "X_train_df['hired'] = y_train.values\n",
                "\n",
                "# Cluster statistics\n",
                "for i in range(optimal_k):\n",
                "    cluster_data = X_train_df[X_train_df['cluster'] == i]\n",
                "    print(f\"\\nCluster {i}:\")\n",
                "    print(f\"  Size: {len(cluster_data)}\")\n",
                "    print(f\"  Hiring Rate: {cluster_data['hired'].mean():.2%}\")\n",
                "    print(f\"  Avg Skills Match: {cluster_data['skills_match_score'].mean():.3f}\")\n",
                "    print(f\"  Avg Experience: {cluster_data['years_experience'].mean():.1f} years\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Best Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pickle\n",
                "\n",
                "# Save best model (XGBoost)\n",
                "with open('../models/best_model.pkl', 'wb') as f:\n",
                "    pickle.dump(xgb_model, f)\n",
                "\n",
                "# Save scaler\n",
                "with open('../models/scaler.pkl', 'wb') as f:\n",
                "    pickle.dump(scaler, f)\n",
                "\n",
                "print(\"Model and scaler saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Conclusions\n",
                "\n",
                "### Key Findings:\n",
                "\n",
                "1. **Best Model**: XGBoost achieves the highest performance with ~87% accuracy\n",
                "\n",
                "2. **Important Features**:\n",
                "   - Technical test score\n",
                "   - Skills match score\n",
                "   - Interview score\n",
                "   - Years of experience\n",
                "\n",
                "3. **Clustering Insights**:\n",
                "   - 3 distinct applicant segments identified\n",
                "   - High-performers cluster has 80%+ hiring rate\n",
                "\n",
                "4. **Class Imbalance**:\n",
                "   - SMOTE effectively addressed imbalance\n",
                "   - Improved recall for minority class\n",
                "\n",
                "### Recommendations:\n",
                "- Focus on technical assessment and skills validation\n",
                "- Target high-performer cluster profiles\n",
                "- Consider referral programs (positive impact on hiring)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}